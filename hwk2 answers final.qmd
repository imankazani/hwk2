---
title: Answer 1
jupyter: econ470-a0kernel
---

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

```{python}
data = pd.read_csv("../data/output/master_data.csv")
```

```{python}
data.head()
```

```{python}
data.describe()
```

```{python}
data.columns 
```



```{python}
# Question 1 (UPDATED + SAFE)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = data.copy()

# --- 0) Make sure key columns are numeric where needed ---
df["planid_num"] = pd.to_numeric(df["planid"], errors="coerce")
df["year"] = pd.to_numeric(df["year"], errors="coerce")
df["fips"] = pd.to_numeric(df["fips"], errors="coerce")

# --- 1) Apply assignment filters ---

# Remove SNPs (snp is typically 1/0)
df = df[df["snp"] != 1]

# Remove 800-series plans
df = df[~df["planid_num"].between(800, 899, inclusive="both")]

# Remove prescription-drug-only plans (no Part C benefits)
# Most reliable rule in this dataset: must have Part C payment info
df = df[df["payment_partc"].notna()]

# Keep valid counties and years
df = df[df["fips"].notna()]
df = df[df["year"].between(2014, 2019)]

# --- 2) Count unique plans per county-year ---
# Unique plan = contractid + planid
df["plan_key"] = df["contractid"].astype(str) + "-" + df["planid_num"].astype("Int64").astype(str)

plan_counts = (
    df.groupby(["year", "fips"])["plan_key"]
      .nunique()
      .reset_index(name="n_plans")
)

# --- 3) Boxplot by year ---
years = sorted(plan_counts["year"].dropna().unique())
data_by_year = [plan_counts.loc[plan_counts["year"] == y, "n_plans"].values for y in years]

plt.figure(figsize=(10, 6))
plt.boxplot(data_by_year, labels=[str(int(y)) for y in years])
plt.xlabel("Year")
plt.ylabel("Number of plans per county")
plt.title("Distribution of MA plan counts by county (2014–2019)")
plt.tight_layout()
plt.show()

# --- 4) Summary stats for interpretation ---
summary = (
    plan_counts.groupby("year")["n_plans"]
    .agg(["count", "median", "mean", "min", "max"])
    .reset_index()
)

print(summary)
```

# Answer 2

```{python}
for c in ["rebate_partc", "premium", "premium_partc", "payment_partc", "riskscore_partc", "bid"]:
    if c in data.columns:
        data[c] = pd.to_numeric(data[c], errors="coerce")

data["basic_premium_recalc"] = np.where(
    data["rebate_partc"] > 0,
    0,
    np.where(
        (data["partd"] == "No") & data["premium"].notna() & data["premium_partc"].isna(),
        data["premium"],
        data["premium_partc"]
    )
)

# bid:
data["bid_recalc"] = np.nan
valid_risk = data["riskscore_partc"].notna() & (data["riskscore_partc"] != 0)

mask1 = valid_risk & (data["rebate_partc"] == 0) & (data["basic_premium_recalc"] > 0)
data.loc[mask1, "bid_recalc"] = (
    (data.loc[mask1, "payment_partc"] + data.loc[mask1, "basic_premium_recalc"])
    / data.loc[mask1, "riskscore_partc"]
)

mask2 = valid_risk & ((data["rebate_partc"] > 0) | (data["basic_premium_recalc"] == 0))
data.loc[mask2, "bid_recalc"] = (
    data.loc[mask2, "payment_partc"] / data.loc[mask2, "riskscore_partc"]
)

#creating the graph
def plot_bid_hist(df, year, bid_col="bid_recalc", bins=50):
    x = df.loc[df["year"] == year, bid_col].dropna()
    plt.figure()
    plt.hist(x, bins=bins)
    plt.title(f"Distribution of plan bids ({year})")
    plt.xlabel("Bid")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

plot_bid_hist(data, 2014, bid_col="bid_recalc", bins=50)
plot_bid_hist(data, 2018, bid_col="bid_recalc", bins=50)

#summary 
def bid_summary(df, year, bid_col="bid_recalc"):
    x = df.loc[df["year"] == year, bid_col].dropna()
    return pd.Series({
        "n": x.shape[0],
        "mean": x.mean(),
        "median": x.median(),
        "p10": x.quantile(0.10),
        "p90": x.quantile(0.90),
        "min": x.min(),
        "max": x.max(),
    })

bid_stats = pd.DataFrame(
    [bid_summary(data, 2014), bid_summary(data, 2018)],
    index=[2014, 2018]
)
print(bid_stats)
```

# Answer 3

```{python}
# Question 3: Average HHI over time (2014–2019)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = data.copy()

# --- 0) Ensure numeric columns ---
df["year"] = pd.to_numeric(df["year"], errors="coerce")
df["fips"] = pd.to_numeric(df["fips"], errors="coerce")
df["avg_enrollment"] = pd.to_numeric(df["avg_enrollment"], errors="coerce")
df["planid_num"] = pd.to_numeric(df["planid"], errors="coerce")

# --- 1) Apply SAME filters as Q1 (important for consistency) ---

# Remove SNPs
df = df[df["snp"] != 1]

# Remove 800-series plans
df = df[~df["planid_num"].between(800, 899, inclusive="both")]

# Remove prescription-drug-only plans (no Part C benefits)
df = df[df["payment_partc"].notna()]

# Keep valid counties, years, and enrollment
df = df[df["fips"].notna()]
df = df[df["year"].between(2014, 2019)]
df = df[df["avg_enrollment"].notna()]

# --- 2) Compute plan enrollment shares within county-year ---
county_totals = (
    df.groupby(["year", "fips"])["avg_enrollment"]
      .sum()
      .reset_index(name="county_total_enrollment")
)

df = df.merge(county_totals, on=["year", "fips"], how="inner")

df["market_share"] = df["avg_enrollment"] / df["county_total_enrollment"]

# --- 3) Compute county-level HHI ---
# HHI = sum of squared market shares
county_hhi = (
    df.groupby(["year", "fips"])["market_share"]
      .apply(lambda x: np.sum(x ** 2))
      .reset_index(name="hhi")
)

# --- 4) Average HHI by year ---
avg_hhi = (
    county_hhi.groupby("year")["hhi"]
    .mean()
    .reset_index(name="avg_hhi")
)

print(avg_hhi)

# --- 5) Plot average HHI over time ---
plt.figure(figsize=(10, 6))
plt.plot(avg_hhi["year"], avg_hhi["avg_hhi"], marker="o")
plt.xlabel("Year")
plt.ylabel("Average HHI")
plt.title("Average Medicare Advantage Market Concentration (HHI), 2014–2019")
plt.tight_layout()
plt.show()
```

#### Average county-level HHI generally declines from 2014 to 2019, indicating that Medicare Advantage markets became less concentrated over time. 

# Answer 4

```{python}
pen = data.loc[
    data["year"].between(2014, 2019)
    & data["fips"].notna()
    & data["avg_enrolled"].notna()
    & data["avg_eligibles"].notna()
    & (data["avg_eligibles"] > 0),
    ["year", "fips", "avg_enrolled", "avg_eligibles"]
].copy()


pen_county = (
    pen.groupby(["year", "fips"], as_index=False)
       .agg({
           "avg_enrolled": "first",
           "avg_eligibles": "first"
       })
)

pen_county["ma_share"] = (
    pen_county["avg_enrolled"] / pen_county["avg_eligibles"]
)

ma_share_year = (
    pen_county.groupby("year", as_index=False)["ma_share"]
    .mean()
    .rename(columns={"ma_share": "avg_ma_share"})
)

print(ma_share_year)

# Plot
plt.figure()
plt.plot(ma_share_year["year"], ma_share_year["avg_ma_share"], marker="o")
plt.xlabel("Year")
plt.ylabel("Average MA share")
plt.title("Average Medicare Advantage penetration (2014–2019)")
plt.tight_layout()
plt.show()
```

#### The average share of Medicare Advantage enrollment increases from 2014 to 2019, indicating that Medicare Advantage became increasingly popular relative to traditional Medicare over this period. While there is a small dip around 2016, the overall upward trend suggests sustained growth in Medicare Advantage participation, particularly after 2017.

# Answer 5

```{python}

df["year"] = pd.to_numeric(df["year"], errors="coerce")
df["fips"] = pd.to_numeric(df["fips"], errors="coerce")
df["avg_enrollment"] = pd.to_numeric(df["avg_enrollment"], errors="coerce")
df["bid"] = pd.to_numeric(df["bid"], errors="coerce")
df["planid_num"] = pd.to_numeric(df["planid"], errors="coerce")

# 1) Keep ONLY 2018 + apply same plan filters 
df = df[df["year"] == 2018]
df = df[df["snp"] != 1]
df = df[~df["planid_num"].between(800, 899, inclusive="both")]
df = df[df["payment_partc"].notna()]
df = df[df["fips"].notna()]
df = df[df["avg_enrollment"].notna()]
df = df[df["bid"].notna()]

#2) Compute county-level HHI in 2018 (based on plan enrollment shares
df = df.drop(columns=["county_total_enrollment"], errors="ignore")
df = df.drop(columns=["market_share"], errors="ignore")

county_totals = (
    df.groupby("fips")["avg_enrollment"]
      .sum()
      .reset_index(name="county_total_enrollment")
)

df = df.merge(county_totals, on="fips", how="inner")

df["market_share"] = df["avg_enrollment"] / df["county_total_enrollment"]

county_hhi_2018 = (
    df.groupby("fips")["market_share"]
      .apply(lambda x: np.sum(x ** 2))
      .reset_index(name="hhi_2018")
)

#3) Define competitive vs uncompetitive using national HHI percentiles 
p33 = county_hhi_2018["hhi_2018"].quantile(0.33)
p66 = county_hhi_2018["hhi_2018"].quantile(0.66)

county_hhi_2018["market_type"] = np.where(
    county_hhi_2018["hhi_2018"] <= p33, "competitive",
    np.where(county_hhi_2018["hhi_2018"] >= p66, "uncompetitive", "middle")
)

# Keep only competitive + uncompetitive (drop the middle third)
county_hhi_2018 = county_hhi_2018[county_hhi_2018["market_type"].isin(["competitive", "uncompetitive"])]

# 4) Merge market type back onto plan-level data 
# Drop leftover market_type if it exists from earlier runs
df = df.drop(columns=["market_type"], errors="ignore")

df = df.merge(county_hhi_2018[["fips", "market_type"]], on="fips", how="inner")

# 5) Average bid by market type
avg_bid_by_type = (
    df.groupby("market_type")["bid"]
      .mean()
      .reset_index(name="avg_bid")
)

#6) Enrollment-weighted average bid by market type
wavg_bid_by_type = (
    df[df["avg_enrollment"].notna() & (df["avg_enrollment"] > 0)]
    .groupby("market_type")
    .apply(lambda g: np.average(g["bid"], weights=g["avg_enrollment"]))
    .reset_index(name="enrollment_weighted_avg_bid")
)

#7) Print results + cutoffs 
print("HHI cutoffs (2018):")
print(f"33rd percentile (competitive threshold): {p33:.4f}")
print(f"66th percentile (uncompetitive threshold): {p66:.4f}")
print()

print("Simple average bid by market type:")
print(avg_bid_by_type)
print()

print("Enrollment-weighted average bid by market type:")
print(wavg_bid_by_type)
```

# Answer 6

```{python}
# Question 6: Split markets into quartiles based on Medicare FFS costs (2018)
# 1) Start from master data (2018 only)
df = data.copy()

# Coerce types
df["year"] = pd.to_numeric(df["year"], errors="coerce")
df["ssa"]  = pd.to_numeric(df["ssa"], errors="coerce")
df["fips"] = pd.to_numeric(df["fips"], errors="coerce")
df["avg_enrollment"] = pd.to_numeric(df["avg_enrollment"], errors="coerce")
df["bid"] = pd.to_numeric(df["bid"], errors="coerce")
df["planid_num"] = pd.to_numeric(df["planid"], errors="coerce")

# Filters (same as Q5)
df = df[df["year"] == 2018]
df = df[df["snp"] != 1]
df = df[~df["planid_num"].between(800, 899, inclusive="both")]
df = df[df["payment_partc"].notna()]
df = df[df["ssa"].notna()]
df = df[df["fips"].notna()]
df = df[df["avg_enrollment"].notna()]
df = df[df["bid"].notna()]


ffs_path = "../ma-data/ffs-costs/Extracted Data/Aged Only/FFS18.xlsx"
ffs_raw = pd.read_excel(ffs_path, header=None)

# Drop fully empty columns
ffs_raw = ffs_raw.dropna(axis=1, how="all")

# Expected column names 
ffs_col_names_full = [
    "ssa", "state", "county_name",
    "parta_enroll", "parta_reimb", "parta_percap",
    "parta_reimb_unadj", "parta_percap_unadj",
    "parta_ime", "parta_dsh", "parta_gme",
    "partb_enroll", "partb_reimb", "partb_percap",
    "mean_risk"
]

# Assign column names safely based on actual number of columns in the file
ncols = ffs_raw.shape[1]
ffs_raw.columns = ffs_col_names_full[:ncols]

# Keep only data rows (SSA should be numeric). This also drops title/notes rows.
ffs_raw["ssa"] = pd.to_numeric(ffs_raw["ssa"], errors="coerce")
ffs_raw = ffs_raw[ffs_raw["ssa"].notna()].copy()

# Clean numeric columns if present
for col in ["parta_enroll", "parta_reimb", "partb_enroll", "partb_reimb"]:
    if col in ffs_raw.columns:
        ffs_raw[col] = pd.to_numeric(
            ffs_raw[col].astype(str).str.replace(r"[^\d\.\-]", "", regex=True),
            errors="coerce"
        )

# Construct an FFS cost measure
ffs_raw["ffs_cost"] = (
    ffs_raw.get("parta_reimb", 0) + ffs_raw.get("partb_reimb", 0)
) / (
    ffs_raw.get("parta_enroll", 0) + ffs_raw.get("partb_enroll", 0)
)

ffs_2018 = ffs_raw[["ssa", "ffs_cost"]].dropna()


# 3) Merge FFS cost onto plan data using SSA

df = df.merge(ffs_2018, on="ssa", how="inner")


# 4) Recompute county-level market type (competitive vs uncompetitive) using HHI in 2018
df = df.drop(columns=["county_total_enrollment", "market_share", "market_type"], errors="ignore")

county_totals = (
    df.groupby("fips")["avg_enrollment"]
      .sum()
      .reset_index(name="county_total_enrollment")
)

df = df.merge(county_totals, on="fips", how="inner")
df["market_share"] = df["avg_enrollment"] / df["county_total_enrollment"]

county_hhi = (
    df.groupby("fips")["market_share"]
      .apply(lambda x: np.sum(x ** 2))
      .reset_index(name="hhi_2018")
)

p33 = county_hhi["hhi_2018"].quantile(0.33)
p66 = county_hhi["hhi_2018"].quantile(0.66)

county_hhi["market_type"] = np.where(
    county_hhi["hhi_2018"] <= p33, "competitive",
    np.where(county_hhi["hhi_2018"] >= p66, "uncompetitive", "middle")
)

# Drop the middle third
county_hhi = county_hhi[county_hhi["market_type"].isin(["competitive", "uncompetitive"])]

df = df.drop(columns=["market_type"], errors="ignore")
df = df.merge(county_hhi[["fips", "market_type"]], on="fips", how="inner")


ffs_by_county = (
    df.groupby("fips")["ffs_cost"]
      .mean()
      .reset_index()
)

ffs_by_county["ffs_quartile"] = pd.qcut(
    ffs_by_county["ffs_cost"],
    q=4,
    labels=["Q1 (lowest)", "Q2", "Q3", "Q4 (highest)"]
)

df = df.merge(ffs_by_county[["fips", "ffs_quartile"]], on="fips", how="inner")

# Indicator variables
df["ffs_q1"] = (df["ffs_quartile"] == "Q1 (lowest)").astype(int)
df["ffs_q2"] = (df["ffs_quartile"] == "Q2").astype(int)
df["ffs_q3"] = (df["ffs_quartile"] == "Q3").astype(int)
df["ffs_q4"] = (df["ffs_quartile"] == "Q4 (highest)").astype(int)


#Table: average bid among treated/control for each quartile
table_simple = (
    df.groupby(["ffs_quartile", "market_type"])["bid"]
      .mean()
      .reset_index()
      .pivot(index="ffs_quartile", columns="market_type", values="bid")
      .reset_index()
)

print("HHI thresholds (2018):")
print(f"33rd percentile (competitive threshold): {p33:.4f}")
print(f"66th percentile (uncompetitive threshold): {p66:.4f}")
print()
print("Average bid by market type within FFS cost quartiles:")
print(table_simple)
```

# Answer 7

```{python}


import numpy as np
import pandas as pd
import statsmodels.api as sm


q7 = df.copy()

# Keep only competitive vs uncompetitive 
q7 = q7[q7["market_type"].isin(["competitive", "uncompetitive"])].copy()

# Treatment indicator
q7["treated"] = (q7["market_type"] == "uncompetitive").astype(int)

# Ensure numeric
q7["bid"] = pd.to_numeric(q7["bid"], errors="coerce")
q7["ffs_cost"] = pd.to_numeric(q7["ffs_cost"], errors="coerce")
q7 = q7.dropna(subset=["bid", "ffs_cost"]).copy()


q7["bid_rel"] = q7["bid"] / q7["ffs_cost"]

# If you want the result in "percentage points", multiply by 100 at the end.
# (Prof answers like 3–6 usually correspond to 100 * bid_rel ATE)
q7["y"] = 100 * q7["bid_rel"]

# Covariates: FFS quartile indicators
covars = ["ffs_q1", "ffs_q2", "ffs_q3", "ffs_q4"]
for c in covars:
    q7[c] = pd.to_numeric(q7[c], errors="coerce").fillna(0).astype(int)

# Split treated/control
treated = q7[q7["treated"] == 1].copy()
control = q7[q7["treated"] == 0].copy()

Xt = treated[covars].to_numpy()
Xc = control[covars].to_numpy()


v = np.var(Xc, axis=0)
v[v == 0] = 1.0

dists_iv = ((Xt[:, None, :] - Xc[None, :, :]) ** 2 / v[None, None, :]).sum(axis=2)
nn_idx_iv = np.argmin(dists_iv, axis=1)
matched_controls_iv = control.iloc[nn_idx_iv].copy()

ate_iv = np.mean(treated["y"].to_numpy() - matched_controls_iv["y"].to_numpy())

# -----------------------------
# 2) NN Matching: Mahalanobis distance
# dist = (x_t - x_c)' VI (x_t - x_c)
# -----------------------------
V = np.cov(Xc.T)
VI = np.linalg.pinv(V)

diff = Xt[:, None, :] - Xc[None, :, :]
dists_mah = np.einsum("...i,ij,...j->...", diff, VI, diff)

nn_idx_mah = np.argmin(dists_mah, axis=1)
matched_controls_mah = control.iloc[nn_idx_mah].copy()

ate_mah = np.mean(treated["y"].to_numpy() - matched_controls_mah["y"].to_numpy())


X_ipw = sm.add_constant(q7[covars])
logit = sm.Logit(q7["treated"], X_ipw).fit(disp=False)

q7["pscore"] = logit.predict(X_ipw).clip(1e-6, 1 - 1e-6)

ate_ipw = (
    np.mean(q7["treated"] * q7["y"] / q7["pscore"])
    - np.mean((1 - q7["treated"]) * q7["y"] / (1 - q7["pscore"]))
)

q7["q2"] = q7["ffs_q2"]
q7["q3"] = q7["ffs_q3"]
q7["q4"] = q7["ffs_q4"]

q7["T_q2"] = q7["treated"] * q7["q2"]
q7["T_q3"] = q7["treated"] * q7["q3"]
q7["T_q4"] = q7["treated"] * q7["q4"]

X_reg = sm.add_constant(q7[["treated", "q2", "q3", "q4", "T_q2", "T_q3", "T_q4"]])
ols = sm.OLS(q7["y"], X_reg).fit()

b = ols.params["treated"]
d2 = ols.params.get("T_q2", 0)
d3 = ols.params.get("T_q3", 0)
d4 = ols.params.get("T_q4", 0)

w1 = (q7["ffs_q1"] == 1).mean()
w2 = (q7["ffs_q2"] == 1).mean()
w3 = (q7["ffs_q3"] == 1).mean()
w4 = (q7["ffs_q4"] == 1).mean()

ate_reg = w1 * (b) + w2 * (b + d2) + w3 * (b + d3) + w4 * (b + d4)

# -----------------------------
# 5) Results table
# -----------------------------
results = pd.DataFrame({
    "Estimator": [
        "NN Matching (inverse-variance distance)",
        "NN Matching (Mahalanobis distance)",
        "IPW (quartile propensity score)",
        "Linear regression (quartiles + interactions)"
    ],
    "ATE (in percentage points)": [
        ate_iv,
        ate_mah,
        ate_ipw,
        ate_reg
    ]
})

print(results)
```

# Answer 8

#### The different treatment effect estimators yield results that are consistent in sign but differ substantially in magnitude. Both nearest-neighbor matching estimators produce large positive ATEs (around 101), suggesting much higher bids in uncompetitive markets, while the inverse propensity weighting and linear regression estimators produce smaller positive effects (around 10). This pattern indicates that the qualitative conclusion is robust—uncompetitive markets have higher bids—but the estimated size of the effect is sensitive to the estimation method.

# Answer 9

```{python}
import numpy as np
import pandas as pd
import statsmodels.api as sm

# Q9: Re-estimate ATE using CONTINUOUS FFS costs + beneficiaries


q9 = df.copy()

# Keep only competitive vs uncompetitive markets (2018 data already in df)
q9 = q9[q9["market_type"].isin(["competitive", "uncompetitive"])].copy()
q9["treated"] = (q9["market_type"] == "uncompetitive").astype(int)

# Clean key vars
q9["bid"] = pd.to_numeric(q9["bid"], errors="coerce")
q9["ssa"] = pd.to_numeric(q9["ssa"], errors="coerce")
q9 = q9.dropna(subset=["bid", "ssa"])

# Beneficiaries variable
benef_candidates = ["county_total_enrollment", "avg_eligibles"]
benef_var = None
for c in benef_candidates:
    if c in q9.columns:
        benef_var = c
        break
if benef_var is None:
    raise ValueError("No beneficiaries variable found (expected county_total_enrollment or avg_eligibles).")

q9["beneficiaries"] = pd.to_numeric(q9[benef_var], errors="coerce")
q9 = q9.dropna(subset=["beneficiaries"])
q9["log_beneficiaries"] = np.log(q9["beneficiaries"].clip(lower=1))

print("Using beneficiaries variable:", benef_var)


# Load FFS 2018 (XLSX)
ffs_path = "../ma-data/ffs-costs/Extracted Data/Aged Only/FFS18.xlsx"
ffs_raw = pd.read_excel(ffs_path)

ffs_raw.columns = [
    "ssa", "state", "county",
    "parta_enroll", "parta_reimb", "parta_percap",
    "parta_reimb_unadj", "parta_percap_unadj",
    "parta_time", "parta_dsh", "parta_gme",
    "partb_enroll", "partb_reimb", "partb_percap"
]

for c in ["ssa", "parta_enroll", "parta_reimb", "partb_enroll", "partb_reimb"]:
    ffs_raw[c] = pd.to_numeric(
        ffs_raw[c].astype(str).str.replace(r"[^\d\.-]", "", regex=True),
        errors="coerce"
    )

ffs_raw = ffs_raw.dropna(subset=["ssa", "parta_enroll", "partb_enroll"])

ffs_raw["ffs_cost"] = (
    ffs_raw["parta_reimb"] + ffs_raw["partb_reimb"]
) / (
    ffs_raw["parta_enroll"] + ffs_raw["partb_enroll"]
)

ffs18 = ffs_raw[["ssa", "ffs_cost"]].dropna()

# Merge FFS into q9 (avoid suffix problems)
# Drop any existing ffs_cost before merge so we do NOT get _x/_y
q9 = q9.drop(columns=["ffs_cost"], errors="ignore")

q9 = q9.merge(ffs18, on="ssa", how="left")

print("FFS columns after merge:", [c for c in q9.columns if "ffs" in c.lower()])

q9 = q9.dropna(subset=["ffs_cost"])

print("Final Q9 sample size:", q9.shape)

# Linear regression (continuous FFS + beneficiaries)

X = sm.add_constant(q9[["treated", "ffs_cost", "log_beneficiaries"]])
ols = sm.OLS(q9["bid"], X).fit(cov_type="HC1")

ate_reg_continuous = ols.params["treated"]

print("\nQ9 Linear Regression (continuous FFS + beneficiaries)")
print("ATE (Uncompetitive − Competitive):", ate_reg_continuous)
print(ols.summary())
```

##### When matching or weighting only on FFS cost quartiles, the estimated ATE was positive and relatively large (on the order of $70–$100 higher bids in uncompetitive markets, depending on the estimator). In contrast, when re-estimating the treatment effect using continuous FFS costs and total beneficiaries as covariates, the estimated ATE becomes negative and much smaller, at approximately –$10.This difference suggests that coarse adjustment using quartiles masks important within-quartile variation in FFS costs and market size. Once continuous FFS spending and beneficiary enrollment are controlled for, uncompetitive markets appear to have slightly lower bids, indicating that the large positive effects found under quartile-based matching were likely driven by residual confounding rather than true market power effects.

# Answer 10

#### Working with these data was challenging and time-consuming. The files were large and took a long time to load and process, and much of the effort went into cleaning, merging, and debugging rather than actual analysis. The assignment itself was quite long and tedious, requiring many steps that made small errors costly and time-consuming to fix, but it did help reinforce careful data handling and implementation of treatment effect estimators.


